<?xml version="1.0" encoding="utf-8"?>
<!-- Copyright © Tanner Gooding and Contributors. Licensed under the MIT License (MIT). See License.md in the repository root for more information. -->
<!-- Ported from https://github.com/MicrosoftDocs/sdk-api/ -->
<doc>
  <member name="DML_ACTIVATION_THRESHOLDED_RELU_OPERATOR_DESC">
    <summary>
      <para>Performs a thresholded rectified linear unit (ReLU) activation function on every element in <i>InputTensor</i>, placing the result into the corresponding element of <i>OutputTensor</i>.</para>
      <code>f(x) = x, if x &gt; Alpha 0, otherwise</code>
      <para>This operator supports in-place execution, meaning that the output tensor is permitted to alias <i>InputTensor</i> during binding.</para>
    </summary>
  </member>
  <member name="DML_ACTIVATION_THRESHOLDED_RELU_OPERATOR_DESC.InputTensor">
    <summary>The input tensor to read from.</summary>
  </member>
  <member name="DML_ACTIVATION_THRESHOLDED_RELU_OPERATOR_DESC.OutputTensor">
    <summary>The output tensor to write the results to.</summary>
  </member>
  <member name="DML_ACTIVATION_THRESHOLDED_RELU_OPERATOR_DESC.Alpha">
    <summary>The threshold for the function. A typical default for this value is 1.0.</summary>
  </member>
</doc>