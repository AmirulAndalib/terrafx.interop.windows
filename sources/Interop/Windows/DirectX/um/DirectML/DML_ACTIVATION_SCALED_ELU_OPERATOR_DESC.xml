<?xml version="1.0" encoding="utf-8"?>
<!-- Copyright © Tanner Gooding and Contributors. Licensed under the MIT License (MIT). See License.md in the repository root for more information. -->
<!-- Ported from https://github.com/MicrosoftDocs/sdk-api/ -->
<doc>
  <member name="DML_ACTIVATION_SCALED_ELU_OPERATOR_DESC">
    <summary>
      <para>Performs a scaled exponential linear unit (ELU) activation function on every element in <i>InputTensor</i>, placing the result into the corresponding element of <i>OutputTensor</i>.</para>
      <code>f(x) = Gamma * x, if x &gt; 0 Gamma * (Alpha * exp(x) - Alpha), otherwise</code>
      <para>Where exp(x) is the natural exponentiation function.</para>
      <para>This operator supports in-place execution, meaning that the output tensor is permitted to alias <i>InputTensor</i> during binding.</para>
    </summary>
  </member>
  <member name="DML_ACTIVATION_SCALED_ELU_OPERATOR_DESC.InputTensor">
    <summary>The input tensor to read from.</summary>
  </member>
  <member name="DML_ACTIVATION_SCALED_ELU_OPERATOR_DESC.OutputTensor">
    <summary>The output tensor to write the results to.</summary>
  </member>
  <member name="DML_ACTIVATION_SCALED_ELU_OPERATOR_DESC.Alpha">
    <summary>The value of alpha. A typical default for this value is 1.6732.</summary>
  </member>
  <member name="DML_ACTIVATION_SCALED_ELU_OPERATOR_DESC.Gamma">
    <summary>The value of gamma. A typical default for this value is 1.0507.</summary>
  </member>
</doc>