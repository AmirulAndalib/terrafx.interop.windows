<?xml version="1.0" encoding="utf-8"?>
<!-- Copyright © Tanner Gooding and Contributors. Licensed under the MIT License (MIT). See License.md in the repository root for more information. -->
<!-- Ported from https://github.com/MicrosoftDocs/sdk-api/ -->
<doc>
  <member name="DML_ACTIVATION_RELU_OPERATOR_DESC">
    <summary>
      <para>Performs a rectified linear unit (ReLU) activation function on every element in <i>InputTensor</i>, placing the result into the corresponding element of <i>OutputTensor</i>.</para>
      <code>f(x) = max(0, x)</code>
      <para>Where max(a,b) returns the larger of the two values a,b.</para>
      <para>This operator supports in-place execution, meaning that the output tensor is permitted to alias <i>InputTensor</i> during binding.</para>
    </summary>
  </member>
  <member name="DML_ACTIVATION_RELU_OPERATOR_DESC.InputTensor">
    <summary>The input tensor to read from.</summary>
  </member>
  <member name="DML_ACTIVATION_RELU_OPERATOR_DESC.OutputTensor">
    <summary>The output tensor to write the results to.</summary>
  </member>
</doc>